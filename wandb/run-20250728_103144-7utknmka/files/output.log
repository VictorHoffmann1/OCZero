[main process] Global storage server has been started from main process.
[main process] Batch storage has been initialized.
[main process] Replay buffer server has been started from main process.
[Watchdog Server] Watchdog server initialized.
[main process] Watchdog server has been started from main process.
[Data worker] Start data worker 0 at process 17819.
[main process] Data workers have all been launched.
[Batch worker GPU] Starting batch worker GPU 0 at process 17819.
[Batch worker GPU] Starting batch worker GPU 1 at process 17819.
[Batch worker GPU] Starting batch worker GPU 2 at process 17819.
[Batch worker GPU] Starting batch worker GPU 3 at process 17819.
[Batch worker GPU] Starting batch worker GPU 4 at process 17819.
[Batch worker GPU] Starting batch worker GPU 5 at process 17819.
[Batch worker GPU] Starting batch worker GPU 6 at process 17819.
[Batch worker GPU] Starting batch worker GPU 7 at process 17819.
[main process] Batch workers have all been launched.
[main process] torch version is 2.7.1+cu126, enabled torch_compile.
[2025-07-28 10:31:45,273][Train][INFO] - config: {'root_dir': '.', 'exp_config': 'components/config/exp/atari.yaml', 'tag': 'EZ-V2', 'debug': False, 'save_path': 'results/Atari/Breakout/EZ-V2-seed=0-2025-07-28 10:31:39/', 'agent_name': 'atari_agent', 'env': {'env': 'Atari', 'game': 'Breakout', 'base_seed': 0, 'n_skip': 4, 'n_stack': 1, 'max_episode_steps': 3000, 'clip_reward': True, 'max_objects': 32, 'use_color': False, 'obs_shape': [32, 6], 'episodic': True, 'action_space_size': 4}, 'rl': {'discount': 0.988053892081, 'unroll_steps': 5, 'td_steps': 5, 'auto_td_steps': 30000, 'td_lambda': 0.95}, 'optimizer': {'type': 'SGD', 'lr': 0.2, 'lr_decay_type': 'none', 'lr_warm_up': 0.01, 'lr_decay_rate': 0.1, 'lr_decay_steps': 100000, 'weight_decay': 0.0001, 'momentum': 0.9}, 'priority': {'use_priority': True, 'priority_prob_alpha': 1.0, 'priority_prob_beta': 1.0, 'min_prior': 1e-06}, 'train': {'load_model_path': '', 'batch_size': 256, 'training_steps': 100000, 'offline_training_steps': 20000, 'start_transitions': 2000, 'eval_n_episode': 10, 'eval_interval': 10000, 'self_play_update_interval': 100, 'reanalyze_update_interval': 200, 'save_ckpt_interval': 10000, 'mini_batch_size': 256, 'reanalyze_ratio': 1.0, 'reward_loss_coeff': 1.0, 'value_loss_coeff': 0.5, 'policy_loss_coeff': 1.0, 'consistency_coeff': 5.0, 'decorrelation_coeff': 0.01, 'off_diag_coeff': 0.005, 'entropy_coeff': 0.005, 'max_grad_norm': 5, 'change_temperature': True, 'periodic_reset': False, 'value_reanalyze': False, 'path_consistency': False, 'use_decorrelation': False, 'value_policy_detach': False, 'optimal_Q': False, 'v_num': 1, 'value_target': 'mixed', 'use_IQL': False, 'IQL_weight': 0.7, 'start_use_mix_training_steps': 30000.0, 'mixed_value_threshold': 5000.0}, 'data': {'num_envs': 4, 'buffer_size': 1000000, 'total_transitions': 100000, 'top_transitions': 200000.0, 'trajectory_size': 400, 'save_video': False, 'save_as_dataset': False}, 'mcts': {'language': 'cython', 'num_simulations': 16, 'num_top_actions': 4, 'c_visit': 50, 'c_scale': 0.1, 'c_base': 19652, 'c_init': 1.25, 'dirichlet_alpha': 0.3, 'explore_frac': 0.25, 'value_minmax_delta': 0.01, 'vis': ['print'], 'mpc_horizon': 1, 'use_gumbel': True}, 'model': {'hidden_dim': 64, 'top_k': 16, 'noisy_net': False, 'state_norm': False, 'value_prefix': True, 'value_target': 'bootstrapped', 'GAE_max_steps': 15, 'init_zero': True, 'projection_layers': [512, 512], 'prjection_head_layers': [128, 512], 'fc_layers': [32], 'lstm_hidden_size': 128, 'lstm_horizon_len': 5, 'policy_loss_type': 'reanalyze', 'reward_support': {'range': [-300, 300], 'scale': 1, 'env': 'Atari', 'bins': 51, 'type': 'support', 'size': 601}, 'value_support': {'range': [-300, 300], 'scale': 1, 'env': 'Atari', 'bins': 51, 'type': 'support', 'size': 601}}, 'actors': {'data_worker': 1, 'batch_worker': 8}, 'wandb': {'project': 'OCZero', 'tag': 'Atari'}, 'augmentation': None, 'ddp': {'world_size': 1, 'training_size': 1, 'address': '127.0.0.1'}, 'ray': {'single_process': False}, 'log': {'log_interval': 100, 'log_smos': False}, 'eval': {'verbose': 0, 'save_path': 'test', 'model_path': 'results/Atari/Breakout/EZ-V2-seed=0-2023-08-10 00:40:53/models/model_10000.p', 'model_path_new': 'results/Atari/Breakout/EZ-V2-seed=0-2023-08-10 00:40:53/models/model_80000.p', 'n_episodes': 10, 'compare_value': False, 'analysis_value': False}}
[2025-07-28 10:31:45,274][Train][INFO] - save model in: results/Atari/Breakout/EZ-V2-seed=0-2025-07-28 10:31:39/models
[36m(EvalWorker pid=18211)[39m [Eval] Start evaluation at step 0.
[36m(DataWorker pid=18209)[39m A.L.E: Arcade Learning Environment (version 0.11.1+2750686)
[36m(DataWorker pid=18209)[39m [Powered by Stella]
[Train] Begin training...
  0%|                                                                                        | 0/120000 [00:00<?, ?it/s]2025-07-28 10:32:38,677	ERROR worker.py:427 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::BatchWorker.run()[39m (pid=18207, ip=172.24.247.44, actor_id=15364786924a5ef4ae2ebd1701000000, repr=<components.worker.batch_worker.BatchWorker object at 0x7f70148c3700>)
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 154, in run
    ray_time = self.make_batch(trained_steps, self.cnt)
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 278, in make_batch
    prepare_func(
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 900, in prepare_reward_value
    state_lst, value_lst, policy_lst = self.efficient_inference(
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 1309, in efficient_inference
    with autocast():
TypeError: __init__() missing 1 required positional argument: 'device_type'
2025-07-28 10:32:38,788	ERROR worker.py:427 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::BatchWorker.run()[39m (pid=18214, ip=172.24.247.44, actor_id=102437c6bfa269b204a5323f01000000, repr=<components.worker.batch_worker.BatchWorker object at 0x7f2aa4543790>)
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 154, in run
    ray_time = self.make_batch(trained_steps, self.cnt)
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 278, in make_batch
    prepare_func(
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 900, in prepare_reward_value
    state_lst, value_lst, policy_lst = self.efficient_inference(
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 1309, in efficient_inference
    with autocast():
TypeError: __init__() missing 1 required positional argument: 'device_type'
2025-07-28 10:32:38,817	ERROR worker.py:427 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::BatchWorker.run()[39m (pid=18206, ip=172.24.247.44, actor_id=25dcb82b4c58f49fb573f3d601000000, repr=<components.worker.batch_worker.BatchWorker object at 0x7f82f3643850>)
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 154, in run
    ray_time = self.make_batch(trained_steps, self.cnt)
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 278, in make_batch
    prepare_func(
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 900, in prepare_reward_value
    state_lst, value_lst, policy_lst = self.efficient_inference(
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 1309, in efficient_inference
    with autocast():
TypeError: __init__() missing 1 required positional argument: 'device_type'
2025-07-28 10:32:38,839	ERROR worker.py:427 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::BatchWorker.run()[39m (pid=18208, ip=172.24.247.44, actor_id=efd8166bb4884dfe4c46ade401000000, repr=<components.worker.batch_worker.BatchWorker object at 0x7efe0cb83700>)
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 154, in run
    ray_time = self.make_batch(trained_steps, self.cnt)
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 278, in make_batch
    prepare_func(
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 900, in prepare_reward_value
    state_lst, value_lst, policy_lst = self.efficient_inference(
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 1309, in efficient_inference
    with autocast():
TypeError: __init__() missing 1 required positional argument: 'device_type'
2025-07-28 10:32:38,965	ERROR worker.py:427 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::BatchWorker.run()[39m (pid=18212, ip=172.24.247.44, actor_id=73b6c52add91cdee2fb19d1601000000, repr=<components.worker.batch_worker.BatchWorker object at 0x7f03add03790>)
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 154, in run
    ray_time = self.make_batch(trained_steps, self.cnt)
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 278, in make_batch
    prepare_func(
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 900, in prepare_reward_value
    state_lst, value_lst, policy_lst = self.efficient_inference(
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 1309, in efficient_inference
    with autocast():
TypeError: __init__() missing 1 required positional argument: 'device_type'
2025-07-28 10:32:39,054	ERROR worker.py:427 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::BatchWorker.run()[39m (pid=18203, ip=172.24.247.44, actor_id=79965b79e4ffc6c799cff8bb01000000, repr=<components.worker.batch_worker.BatchWorker object at 0x7ff7ed743790>)
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 154, in run
    ray_time = self.make_batch(trained_steps, self.cnt)
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 278, in make_batch
    prepare_func(
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 900, in prepare_reward_value
    state_lst, value_lst, policy_lst = self.efficient_inference(
  File "/home/victo/projects/EfficientZeroV2/components/worker/batch_worker.py", line 1309, in efficient_inference
    with autocast():
TypeError: __init__() missing 1 required positional argument: 'device_type'
[33m(raylet)[39m [2025-07-28 10:32:41,926 E 18084 18084] (raylet) node_manager.cc:3041: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: d74a02954680bd52be461592d7684cf7708cc8edc10e5b7b9a846d19, IP: 172.24.247.44) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.247.44`
[33m(raylet)
[33m(raylet)[39m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[36m(EvalWorker pid=18211)[39m A.L.E: Arcade Learning Environment (version 0.11.1+2750686)
[36m(EvalWorker pid=18211)[39m [Powered by Stella]
Traceback (most recent call last):
  File "/home/victo/projects/EfficientZeroV2/components/train.py", line 164, in <module>
    main()
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/victo/projects/EfficientZeroV2/components/train.py", line 49, in main
    start_ddp_trainer(0, config)
  File "/home/victo/projects/EfficientZeroV2/components/train.py", line 87, in start_ddp_trainer
    final_weights = train(rank, agent, manager, logger, config)
  File "/home/victo/projects/EfficientZeroV2/components/train.py", line 111, in train
    final_weights, final_model = agent.train(
  File "/home/victo/projects/EfficientZeroV2/components/agents/base.py", line 198, in train
    time.sleep(0.3)
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/victo/projects/EfficientZeroV2/components/train.py", line 164, in <module>
    main()
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/victo/projects/EfficientZeroV2/components/train.py", line 49, in main
    start_ddp_trainer(0, config)
  File "/home/victo/projects/EfficientZeroV2/components/train.py", line 87, in start_ddp_trainer
    final_weights = train(rank, agent, manager, logger, config)
  File "/home/victo/projects/EfficientZeroV2/components/train.py", line 111, in train
    final_weights, final_model = agent.train(
  File "/home/victo/projects/EfficientZeroV2/components/agents/base.py", line 198, in train
    time.sleep(0.3)
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 239, in shutdown
    self.process.wait(300)
  File "/home/victo/.pyenv/versions/3.9.22/lib/python3.9/subprocess.py", line 1189, in wait
    return self._wait(timeout=timeout)
  File "/home/victo/.pyenv/versions/3.9.22/lib/python3.9/subprocess.py", line 1927, in _wait
    time.sleep(delay)
KeyboardInterrupt