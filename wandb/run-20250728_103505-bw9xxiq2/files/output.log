[main process] Global storage server has been started from main process.
[main process] Batch storage has been initialized.
[main process] Replay buffer server has been started from main process.
[Watchdog Server] Watchdog server initialized.
[main process] Watchdog server has been started from main process.
[Data worker] Start data worker 0 at process 23642.
[main process] Data workers have all been launched.
[Batch worker GPU] Starting batch worker GPU 0 at process 23642.
[Batch worker GPU] Starting batch worker GPU 1 at process 23642.
[Batch worker GPU] Starting batch worker GPU 2 at process 23642.
[Batch worker GPU] Starting batch worker GPU 3 at process 23642.
[Batch worker GPU] Starting batch worker GPU 4 at process 23642.
[Batch worker GPU] Starting batch worker GPU 5 at process 23642.
[Batch worker GPU] Starting batch worker GPU 6 at process 23642.
[Batch worker GPU] Starting batch worker GPU 7 at process 23642.
[main process] Batch workers have all been launched.
[main process] torch version is 2.7.1+cu126, enabled torch_compile.
[2025-07-28 10:35:06,721][Train][INFO] - config: {'root_dir': '.', 'exp_config': 'components/config/exp/atari.yaml', 'tag': 'EZ-V2', 'debug': False, 'save_path': 'results/Atari/Breakout/EZ-V2-seed=0-2025-07-28 10:35:02/', 'agent_name': 'atari_agent', 'env': {'env': 'Atari', 'game': 'Breakout', 'base_seed': 0, 'n_skip': 4, 'n_stack': 1, 'max_episode_steps': 3000, 'clip_reward': True, 'max_objects': 32, 'use_color': False, 'obs_shape': [32, 6], 'episodic': True, 'action_space_size': 4}, 'rl': {'discount': 0.988053892081, 'unroll_steps': 5, 'td_steps': 5, 'auto_td_steps': 30000, 'td_lambda': 0.95}, 'optimizer': {'type': 'SGD', 'lr': 0.2, 'lr_decay_type': 'none', 'lr_warm_up': 0.01, 'lr_decay_rate': 0.1, 'lr_decay_steps': 100000, 'weight_decay': 0.0001, 'momentum': 0.9}, 'priority': {'use_priority': True, 'priority_prob_alpha': 1.0, 'priority_prob_beta': 1.0, 'min_prior': 1e-06}, 'train': {'load_model_path': '', 'batch_size': 256, 'training_steps': 100000, 'offline_training_steps': 20000, 'start_transitions': 2000, 'eval_n_episode': 10, 'eval_interval': 10000, 'self_play_update_interval': 100, 'reanalyze_update_interval': 200, 'save_ckpt_interval': 10000, 'mini_batch_size': 256, 'reanalyze_ratio': 1.0, 'reward_loss_coeff': 1.0, 'value_loss_coeff': 0.5, 'policy_loss_coeff': 1.0, 'consistency_coeff': 5.0, 'decorrelation_coeff': 0.01, 'off_diag_coeff': 0.005, 'entropy_coeff': 0.005, 'max_grad_norm': 5, 'change_temperature': True, 'periodic_reset': False, 'value_reanalyze': False, 'path_consistency': False, 'use_decorrelation': False, 'value_policy_detach': False, 'optimal_Q': False, 'v_num': 1, 'value_target': 'mixed', 'use_IQL': False, 'IQL_weight': 0.7, 'start_use_mix_training_steps': 30000.0, 'mixed_value_threshold': 5000.0}, 'data': {'num_envs': 4, 'buffer_size': 1000000, 'total_transitions': 100000, 'top_transitions': 200000.0, 'trajectory_size': 400, 'save_video': False, 'save_as_dataset': False}, 'mcts': {'language': 'cython', 'num_simulations': 16, 'num_top_actions': 4, 'c_visit': 50, 'c_scale': 0.1, 'c_base': 19652, 'c_init': 1.25, 'dirichlet_alpha': 0.3, 'explore_frac': 0.25, 'value_minmax_delta': 0.01, 'vis': ['print'], 'mpc_horizon': 1, 'use_gumbel': True}, 'model': {'hidden_dim': 64, 'top_k': 16, 'noisy_net': False, 'state_norm': False, 'value_prefix': True, 'value_target': 'bootstrapped', 'GAE_max_steps': 15, 'init_zero': True, 'projection_layers': [512, 512], 'prjection_head_layers': [128, 512], 'fc_layers': [32], 'lstm_hidden_size': 128, 'lstm_horizon_len': 5, 'policy_loss_type': 'reanalyze', 'reward_support': {'range': [-300, 300], 'scale': 1, 'env': 'Atari', 'bins': 51, 'type': 'support', 'size': 601}, 'value_support': {'range': [-300, 300], 'scale': 1, 'env': 'Atari', 'bins': 51, 'type': 'support', 'size': 601}}, 'actors': {'data_worker': 1, 'batch_worker': 8}, 'wandb': {'project': 'OCZero', 'tag': 'Atari'}, 'augmentation': None, 'ddp': {'world_size': 1, 'training_size': 1, 'address': '127.0.0.1'}, 'ray': {'single_process': False}, 'log': {'log_interval': 100, 'log_smos': False}, 'eval': {'verbose': 0, 'save_path': 'test', 'model_path': 'results/Atari/Breakout/EZ-V2-seed=0-2023-08-10 00:40:53/models/model_10000.p', 'model_path_new': 'results/Atari/Breakout/EZ-V2-seed=0-2023-08-10 00:40:53/models/model_80000.p', 'n_episodes': 10, 'compare_value': False, 'analysis_value': False}}
[2025-07-28 10:35:06,722][Train][INFO] - save model in: results/Atari/Breakout/EZ-V2-seed=0-2025-07-28 10:35:02/models
[36m(EvalWorker pid=24005)[39m [Eval] Start evaluation at step 0.
[36m(DataWorker pid=23994)[39m A.L.E: Arcade Learning Environment (version 0.11.1+2750686)
[36m(DataWorker pid=23994)[39m [Powered by Stella]
  0%|                                                                                        | 0/120000 [00:00<?, ?it/s]
  0%|                                                                                        | 0/120000 [00:00<?, ?it/s][33m(raylet)[39m [2025-07-28 10:36:03,463 E 23901 23901] (raylet) node_manager.cc:3041: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: df286da01e45c52f416c430737b5ce97ba8a5eb67cf1d6da51e5a7fc, IP: 172.24.247.44) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.24.247.44`
[33m(raylet)
[33m(raylet)[39m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[36m(EvalWorker pid=24005)[39m A.L.E: Arcade Learning Environment (version 0.11.1+2750686)
[36m(EvalWorker pid=24005)[39m [Powered by Stella]
/home/victo/projects/EfficientZeroV2/components/agents/base.py:473: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  obs_batch_raw = torch.from_numpy(obs_batch_ori).cuda().float()
Error in training:
[main process] master worker failed
Error executing job with overrides: ['exp_config=components/config/exp/atari.yaml']
Traceback (most recent call last):
  File "/home/victo/projects/EfficientZeroV2/components/train.py", line 111, in train
    final_weights, final_model = agent.train(
  File "/home/victo/projects/EfficientZeroV2/components/agents/base.py", line 235, in train
    scalers, log_data = self.update_weights(
  File "/home/victo/projects/EfficientZeroV2/components/agents/base.py", line 519, in update_weights
    assert obs_target_batch.shape[1] == 1
AssertionError
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "/home/victo/projects/EfficientZeroV2/components/train.py", line 164, in <module>
    main()
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/victo/projects/EfficientZeroV2/components/train.py", line 49, in main
    start_ddp_trainer(0, config)
  File "/home/victo/projects/EfficientZeroV2/components/train.py", line 87, in start_ddp_trainer
    final_weights = train(rank, agent, manager, logger, config)
  File "/home/victo/projects/EfficientZeroV2/components/train.py", line 157, in train
    join_workers(workers, server_lst)
  File "/home/victo/projects/EfficientZeroV2/components/worker/__init__.py", line 92, in join_workers
    data_worker.join()
AttributeError: 'NoneType' object has no attribute 'join'
Traceback (most recent call last):
  File "/home/victo/projects/EfficientZeroV2/components/train.py", line 111, in train
    final_weights, final_model = agent.train(
  File "/home/victo/projects/EfficientZeroV2/components/agents/base.py", line 235, in train
    scalers, log_data = self.update_weights(
  File "/home/victo/projects/EfficientZeroV2/components/agents/base.py", line 519, in update_weights
    assert obs_target_batch.shape[1] == 1
AssertionError
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "/home/victo/projects/EfficientZeroV2/components/train.py", line 164, in <module>
    main()
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/victo/.pyenv/versions/oc_atari_rl/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/victo/projects/EfficientZeroV2/components/train.py", line 49, in main
    start_ddp_trainer(0, config)
  File "/home/victo/projects/EfficientZeroV2/components/train.py", line 87, in start_ddp_trainer
    final_weights = train(rank, agent, manager, logger, config)
  File "/home/victo/projects/EfficientZeroV2/components/train.py", line 157, in train
    join_workers(workers, server_lst)
  File "/home/victo/projects/EfficientZeroV2/components/worker/__init__.py", line 92, in join_workers
    data_worker.join()
AttributeError: 'NoneType' object has no attribute 'join'